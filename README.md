ü§ñ Intelligent Operations Data Pipeline (AWS Serverless + ML)
<img width="1920" height="1080" alt="awsIsolateDash" src="https://github.com/user-attachments/assets/2b0961eb-a2ad-4e6b-88dc-5517687c50f9" />

This project showcases a Full-Stack Operations Engineer's ability to deploy machine learning models in a serverless environment to enhance quality control and process monitoring. It is architected for maximum efficiency and strict compliance with the AWS Free Tier.
<img width="1920" height="1080" alt="AWSmonitor" src="https://github.com/user-attachments/assets/d5392396-b369-4501-9e02-aeb664ec2eaa" />
<img width="1920" height="1080" alt="awsIcket" src="https://github.com/user-attachments/assets/d3602721-f63b-4ac6-aaae-7162b922bd3a" />
<img width="1920" height="1080" alt="awsDarkDash" src="https://github.com/user-attachments/assets/05fb8de4-e303-4108-abdf-791da9ec7dae" />

Development Note: This project was developed with the assistance of Gemini and GitHub Copilot for rapid code generation and troubleshooting, demonstrating proficiency with modern AI-driven development workflows.

1. Project Goal & Architecture

Objective: To take simulated raw manufacturing logs, use a pre-trained Machine Learning model to assess the Failure Risk of each component, and display the aggregated results in a sortable, interactive web dashboard hosted for free.

Key Skills Demonstrated:

Machine Learning Integration: Deploying a serialized Python ML model within AWS Lambda.

Process Engineering: Defining risk metrics and setting up automated alerts.

Data Visualization: Creating a client-side, interactive graph for data triage and sorting.

Cloud Architecture: Serverless design for cost-efficiency (S3, Lambda, SNS).

2. Local Setup: Data & ML Model Generation

‚ö†Ô∏è IMPORTANT: Python Environment Setup (Linux/macOS)

Before running any scripts, set up a Python virtual environment:

```bash
# Create virtual environment (one-time setup)
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install required packages
pip install pandas scikit-learn

# Run scripts
python generate_logs.py
python train_model.py
```

Alternatively, run without activating:
```bash
.venv/bin/python generate_logs.py
.venv/bin/python train_model.py
```

We must create two files locally before deploying anything: the data generator and the ML model script.

A. Data Generator (generate_logs.py)

‚úÖ STATUS: COMPLETED

Instructions for Copilot:

Goal: Create a Python script named generate_logs.py that outputs a large CSV file (raw_manufacturing_logs.csv) with a new column: historical_failure_status.

Requirements:

Generate 5,000 records.

Include columns: timestamp, serial_number, vibration_level, component_temp_C, and a new boolean target column: historical_failure_status (0 for normal, 1 for failure, based on simulated high temp/vibration).

Crucial Test Data: Include approximately 5% of records with artificially high temp/vibration that correspond to a historical_failure_status of 1.

Cost Control: Ensure no excessive printing of records; only print a final success summary.

Results:
- ‚úì Generated 5,000 records with 250 failure cases (5.0%)
- ‚úì Output file: raw_manufacturing_logs.csv
- ‚úì Failure thresholds: Temp ‚â•95¬∞C, Vibration ‚â•4.0

B. Machine Learning Model (train_model.py)

The ML model will be a simple Logistic Regression Classifier using scikit-learn to predict the likelihood of failure based on temperature and vibration. This model must be serialized (saved) so Lambda can load it without retraining.

‚úÖ STATUS: COMPLETED

Instructions for Copilot:

Goal: Create a Python script named train_model.py that performs simple ML training and serialization.

Requirements:

Load the data generated by generate_logs.py.

Use scikit-learn (Logistic Regression Classifier) to predict historical_failure_status based on component_temp_C and vibration_level.

Serialize (save) the trained model using Python's pickle library to a file named failure_risk_model.pkl.

Cost Control: Ensure the scikit-learn library and the pickled model are included in the Lambda deployment package (this will slightly increase the package size, but it is necessary for deployment).

Results:
- ‚úì Model trained on 5,000 records (80/20 train/test split)
- ‚úì Model accuracy: 100.00% on test data
- ‚úì Model serialized to: failure_risk_model.pkl
- ‚úì Ready for Lambda deployment

3. AWS Lambda Function: The Intelligent Validator

This is the core Python logic deployed to AWS Lambda (Python 3.12 runtime). It must handle data, run the ML prediction, and manage free tier restrictions.

‚úÖ STATUS: COMPLETED (Ready for Deployment)

Instructions for Copilot:

Goal: Create a Python script named lambda_function.py containing the lambda_handler function.

Requirements (Logic):

Upon invocation, the function must load the failure_risk_model.pkl file from its deployment package.

Iterate through the log data (simulate reading the CSV).

Validation: Perform the standard validation (type conversion, checking against MIN/MAX_TOLERANCE).

ML Prediction: Use the loaded ML model to predict the failure_risk_score (probability between 0 and 1) for each incoming record.

Output: Return a single, structured JSON payload containing a list of the validated records (including the new failure_risk_score) and the overall pass/fail counts.

Cost Control (CRITICAL):

Minimize the use of print() statements to only include high-level summary/alert information. Do not print every record processed.

Use efficient data structures to minimize Lambda processing time.

Implementation Notes:
- ‚úì Lambda function created with ML model loading capability
- ‚úì Validates temperature (50-120¬∞C) and vibration (0-10) ranges
- ‚úì Predicts failure risk scores (0-1 probability) using loaded model
- ‚úì Returns structured JSON with validated records sorted by risk score
- ‚úì CORS-enabled for GitHub Pages integration
- ‚úì Minimal logging (summary only, no individual record printing)
- ‚úì Includes top 10 highest-risk components in response

4. Frontend Visualization (GitHub Pages)

üìã STATUS: PENDING (Next Phase)

The static frontend will be hosted for free on GitHub Pages and use client-side JavaScript to fetch the validated data and create an interactive display.

Instructions for Copilot:

Goal: Create a single index.html file that serves as the visual dashboard.

Requirements (Visualization):

Use Chart.js (via CDN) to create a scatter plot or bar chart that visualizes the validated data.

Interactive Feature: The JavaScript must include a function to allow the user to sort the displayed data by the failure_risk_score (highest risk first) and re-render the chart/table.

Display a summary table showing the top 10 highest-risk components identified by the ML model.

Cost Control: All data fetching, filtering, and plotting must be done client-side in JavaScript. No server-side rendering or database lookups are allowed.

5. AWS Free Tier Compliance Checklist

YOU MUST MANUALLY APPLY THESE RULES IN THE AWS CONSOLE:

AWS Service

Cost Control Feature

Goal

CloudWatch Logs

Log Retention: Set to 7 Days for the Lambda Log Group.

Prevents excessive log storage/ingestion fees.

Amazon S3

Lifecycle Rule: Automatically delete all objects (logs/data) older than 90 days.

Prevents S3 storage fees after the initial 5GB/12-month free tier.

Billing & Budgets

Billing Alarm: Set a monthly budget of $1.00 with an alert at 50%.

Essential safety net to prevent unexpected charges.

Lambda Code

Minimal Print Statements: Confirmed in the code to ensure log volume remains tiny.

Ensures the core compute and logging costs remain Always Free.

---

## üì¶ Project Files Status

### ‚úÖ Completed Local Development

#### Core Application Files
- **generate_logs.py** - Data generation script (5,000 records with 5% failure rate)
- **train_model.py** - ML model training and serialization (100% accuracy)
- **lambda_function.py** - Lambda handler with ML prediction logic
- **index.html** - Interactive frontend dashboard with Chart.js visualization

#### Generated Files (Not in Git)
- **raw_manufacturing_logs.csv** - Generated training data (250 failures)
- **failure_risk_model.pkl** - Serialized Logistic Regression model
- **.venv/** - Python virtual environment (pandas, scikit-learn)

#### Configuration & Documentation
- **README.md** - Complete project documentation
- **DEPLOYMENT.md** - AWS Lambda deployment guide
- **requirements.txt** - Python dependencies (pandas, scikit-learn)
- **.gitignore** - Git exclusions (venv, csv, pkl files)
- **setup.sh** - Automated local setup script

### üìã Ready for Repository Commit
All essential files are created and tested. You can now:
1. Initialize Git repository
2. Make first commit
3. Push to GitHub
4. Continue development in GitHub Codespace

### üöÄ Pending Deployment
- **Lambda Deployment Package** - Bundle lambda_function.py + failure_risk_model.pkl + dependencies
- **AWS Configuration** - Lambda, API Gateway, CloudWatch, S3 setup
- **GitHub Pages** - Enable for index.html hosting

### üîß Development Environment
- Python 3.12.3
- Virtual Environment: `.venv/`
- Dependencies: pandas, scikit-learn
- AI Tools: Gemini & GitHub Copilot assisted development

---

## üöÄ Next Steps

### Ready for First Commit! ‚úÖ

Your project is now complete for the initial repository commit. All core files are ready:

```bash
# Quick test of the dashboard
# Open index.html in your browser to see the interactive ML dashboard

# Initialize Git repository
git init
git add .
git commit -m "Initial commit: AWS Serverless ML Pipeline with complete frontend"

# Create GitHub repository and push
git remote add origin https://github.com/YOUR_USERNAME/aws-sensor-ml-pipeline.git
git branch -M main
git push -u origin main
```

### Moving to GitHub Codespace

Once your repository is on GitHub:

1. **Open in Codespace**: Click "Code" ‚Üí "Codespaces" ‚Üí "Create codespace on main"
2. **Auto Setup**: Run `./setup.sh` to configure the environment
3. **Continue Development**:
   - Package Lambda deployment
   - Configure AWS services
   - Enable GitHub Pages for dashboard
   - Set up API Gateway for frontend-backend integration

### Local Development Workflow

```bash
# Activate virtual environment
source .venv/bin/activate

# Regenerate data and retrain model
python generate_logs.py
python train_model.py

# Test Lambda function locally
python lambda_function.py

# View dashboard
# Open index.html in browser
```

### AWS Deployment Checklist

1. **Create Frontend Dashboard** ‚úÖ (index.html with Chart.js)
2. **Initialize Git Repository** (Ready to execute)
3. **Create GitHub Repository** (Ready to execute)
4. **Package Lambda Function** (See DEPLOYMENT.md)
5. **Deploy to AWS** (Lambda, API Gateway, CloudWatch)
6. **Enable GitHub Pages** (Settings ‚Üí Pages ‚Üí Deploy from main branch)
7. **Configure AWS Free Tier Safeguards** (billing alerts, log retention, lifecycle policies)
